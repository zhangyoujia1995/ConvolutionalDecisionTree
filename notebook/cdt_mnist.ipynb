{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import solver, cdt\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose([transforms.Resize((64, 64)),transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./MNIST/', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Convolution Decision Tree Example(MNIST)')\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                    help='input batch size for training (default: 100)')\n",
    "parser.add_argument('--input-nc', type=int, default=1, metavar='N',\n",
    "                    help='input number of channel(default: 1)')\n",
    "parser.add_argument('--input-dim', type=int, default=28*28, metavar='N',\n",
    "                    help='input dimension size(default: 28*28)')\n",
    "parser.add_argument('--input-height', type=int, default=28, metavar='N',\n",
    "                    help='input height size(default: 28)')\n",
    "parser.add_argument('--output-dim', type=int, default=10, metavar='N',\n",
    "                    help='output dimension size(default: 10)')\n",
    "parser.add_argument('--max-depth', type=int, default=3, metavar='N',\n",
    "                    help='maximum depth of tree(default: 3)')\n",
    "parser.add_argument('--lmbda', type=float, default=0.01, metavar='LR',\n",
    "                    help='temperature rate (default: 0.01)')\n",
    "parser.add_argument('--n-tree', type=int, default=3, metavar='N',\n",
    "                    help='number of trees for CDForest(default: 3)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "\n",
    "parser.add_argument('--max-epoch', type=int, default=100, metavar='N',\n",
    "                    help='number of epochs to train (default: 20)')\n",
    "parser.add_argument('--disp-freq', type=int, default=100, metavar='N',\n",
    "                    help='display frequency')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='S',\n",
    "                    help='random seed (default: 0)')\n",
    "parser.add_argument('-f') # dont know why, but useful\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING WITH GPU\n"
     ]
    }
   ],
   "source": [
    "model = cdt.CDTree(args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    print('RUNNING WITH GPU')\n",
    "else:\n",
    "    model.cpu()\n",
    "    print(\"WARNING: RUNNING WITHOUT GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1][100]\t Batch [500][500]\t Training Loss 0.0366\t Accuracy 0.9900\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0835\t Average training accuracy 0.9611\n",
      "Epoch [1]\t Average validation loss 0.0525\t Average validation accuracy 0.9772\n",
      "\n",
      "Epoch [2][100]\t Batch [500][500]\t Training Loss 0.0747\t Accuracy 0.9700\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0437\t Average training accuracy 0.9819\n",
      "Epoch [2]\t Average validation loss 0.0545\t Average validation accuracy 0.9772\n",
      "\n",
      "Epoch [3][100]\t Batch [500][500]\t Training Loss 0.0248\t Accuracy 0.9900\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0373\t Average training accuracy 0.9844\n",
      "Epoch [3]\t Average validation loss 0.0421\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [4][100]\t Batch [500][500]\t Training Loss 0.0290\t Accuracy 0.9900\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0350\t Average training accuracy 0.9856\n",
      "Epoch [4]\t Average validation loss 0.0426\t Average validation accuracy 0.9823\n",
      "\n",
      "Epoch [5][100]\t Batch [500][500]\t Training Loss 0.0617\t Accuracy 0.9700\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0346\t Average training accuracy 0.9850\n",
      "Epoch [5]\t Average validation loss 0.0415\t Average validation accuracy 0.9832\n",
      "\n",
      "Epoch [6][100]\t Batch [500][500]\t Training Loss 0.0228\t Accuracy 0.9900\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0322\t Average training accuracy 0.9862\n",
      "Epoch [6]\t Average validation loss 0.0464\t Average validation accuracy 0.9807\n",
      "\n",
      "Epoch [7][100]\t Batch [500][500]\t Training Loss 0.0222\t Accuracy 0.9800\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0337\t Average training accuracy 0.9852\n",
      "Epoch [7]\t Average validation loss 0.0400\t Average validation accuracy 0.9828\n",
      "\n",
      "Epoch [8][100]\t Batch [500][500]\t Training Loss 0.0222\t Accuracy 0.9900\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0300\t Average training accuracy 0.9878\n",
      "Epoch [8]\t Average validation loss 0.0446\t Average validation accuracy 0.9827\n",
      "\n",
      "Epoch [9][100]\t Batch [500][500]\t Training Loss 0.0220\t Accuracy 0.9800\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0298\t Average training accuracy 0.9869\n",
      "Epoch [9]\t Average validation loss 0.0373\t Average validation accuracy 0.9827\n",
      "\n",
      "Epoch [10][100]\t Batch [500][500]\t Training Loss 0.0313\t Accuracy 0.9700\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0290\t Average training accuracy 0.9877\n",
      "Epoch [10]\t Average validation loss 0.0385\t Average validation accuracy 0.9827\n",
      "\n",
      "Epoch [11][100]\t Batch [500][500]\t Training Loss 0.0113\t Accuracy 1.0000\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0282\t Average training accuracy 0.9878\n",
      "Epoch [11]\t Average validation loss 0.0341\t Average validation accuracy 0.9846\n",
      "\n",
      "Epoch [12][100]\t Batch [500][500]\t Training Loss 0.0181\t Accuracy 0.9800\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0274\t Average training accuracy 0.9886\n",
      "Epoch [12]\t Average validation loss 0.0341\t Average validation accuracy 0.9849\n",
      "\n",
      "Epoch [13][100]\t Batch [500][500]\t Training Loss 0.0582\t Accuracy 0.9700\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0257\t Average training accuracy 0.9896\n",
      "Epoch [13]\t Average validation loss 0.0342\t Average validation accuracy 0.9855\n",
      "\n",
      "Epoch [14][100]\t Batch [500][500]\t Training Loss 0.0407\t Accuracy 0.9600\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0271\t Average training accuracy 0.9882\n",
      "Epoch [14]\t Average validation loss 0.0502\t Average validation accuracy 0.9731\n",
      "\n",
      "Epoch [15][100]\t Batch [500][500]\t Training Loss 0.0289\t Accuracy 0.9700\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0264\t Average training accuracy 0.9877\n",
      "Epoch [15]\t Average validation loss 0.0353\t Average validation accuracy 0.9829\n",
      "\n",
      "Epoch [16][100]\t Batch [500][500]\t Training Loss 0.0463\t Accuracy 0.9900\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0264\t Average training accuracy 0.9889\n",
      "Epoch [16]\t Average validation loss 0.0358\t Average validation accuracy 0.9846\n",
      "\n",
      "Epoch [17][100]\t Batch [500][500]\t Training Loss 0.0138\t Accuracy 0.9900\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0255\t Average training accuracy 0.9894\n",
      "Epoch [17]\t Average validation loss 0.0337\t Average validation accuracy 0.9849\n",
      "\n",
      "Epoch [18][100]\t Batch [500][500]\t Training Loss 0.0297\t Accuracy 0.9900\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0243\t Average training accuracy 0.9898\n",
      "Epoch [18]\t Average validation loss 0.0343\t Average validation accuracy 0.9849\n",
      "\n",
      "Epoch [19][100]\t Batch [500][500]\t Training Loss 0.0301\t Accuracy 1.0000\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0252\t Average training accuracy 0.9895\n",
      "Epoch [19]\t Average validation loss 0.0375\t Average validation accuracy 0.9847\n",
      "\n",
      "Epoch [20][100]\t Batch [500][500]\t Training Loss 0.0286\t Accuracy 0.9900\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0244\t Average training accuracy 0.9891\n",
      "Epoch [20]\t Average validation loss 0.0339\t Average validation accuracy 0.9848\n",
      "\n",
      "Epoch [21][100]\t Batch [500][500]\t Training Loss 0.0261\t Accuracy 0.9900\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0239\t Average training accuracy 0.9900\n",
      "Epoch [21]\t Average validation loss 0.0295\t Average validation accuracy 0.9877\n",
      "\n",
      "Epoch [22][100]\t Batch [500][500]\t Training Loss 0.0234\t Accuracy 0.9900\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0247\t Average training accuracy 0.9891\n",
      "Epoch [22]\t Average validation loss 0.0335\t Average validation accuracy 0.9847\n",
      "\n",
      "Epoch [23][100]\t Batch [500][500]\t Training Loss 0.0250\t Accuracy 0.9900\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0235\t Average training accuracy 0.9900\n",
      "Epoch [23]\t Average validation loss 0.0369\t Average validation accuracy 0.9822\n",
      "\n",
      "Epoch [24][100]\t Batch [500][500]\t Training Loss 0.0163\t Accuracy 1.0000\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0239\t Average training accuracy 0.9903\n",
      "Epoch [24]\t Average validation loss 0.0308\t Average validation accuracy 0.9860\n",
      "\n",
      "Epoch [25][100]\t Batch [500][500]\t Training Loss 0.0103\t Accuracy 1.0000\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0241\t Average training accuracy 0.9898\n",
      "Epoch [25]\t Average validation loss 0.0354\t Average validation accuracy 0.9828\n",
      "\n",
      "Epoch [26][100]\t Batch [500][500]\t Training Loss 0.0795\t Accuracy 0.9800\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0238\t Average training accuracy 0.9899\n",
      "Epoch [26]\t Average validation loss 0.0363\t Average validation accuracy 0.9846\n",
      "\n",
      "Epoch [27][100]\t Batch [500][500]\t Training Loss 0.0190\t Accuracy 1.0000\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0240\t Average training accuracy 0.9899\n",
      "Epoch [27]\t Average validation loss 0.0340\t Average validation accuracy 0.9857\n",
      "\n",
      "Epoch [28][100]\t Batch [500][500]\t Training Loss 0.0261\t Accuracy 0.9900\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0238\t Average training accuracy 0.9897\n",
      "Epoch [28]\t Average validation loss 0.0310\t Average validation accuracy 0.9861\n",
      "\n",
      "Epoch [29][100]\t Batch [500][500]\t Training Loss 0.0179\t Accuracy 0.9900\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0233\t Average training accuracy 0.9894\n",
      "Epoch [29]\t Average validation loss 0.0343\t Average validation accuracy 0.9869\n",
      "\n",
      "Epoch [30][100]\t Batch [500][500]\t Training Loss 0.0310\t Accuracy 0.9800\n",
      "\n",
      "Epoch [30]\t Average training loss 0.0237\t Average training accuracy 0.9899\n",
      "Epoch [30]\t Average validation loss 0.0386\t Average validation accuracy 0.9831\n",
      "\n",
      "Epoch [31][100]\t Batch [500][500]\t Training Loss 0.0175\t Accuracy 0.9900\n",
      "\n",
      "Epoch [31]\t Average training loss 0.0228\t Average training accuracy 0.9902\n",
      "Epoch [31]\t Average validation loss 0.0322\t Average validation accuracy 0.9857\n",
      "\n",
      "Epoch [32][100]\t Batch [500][500]\t Training Loss 0.0204\t Accuracy 0.9900\n",
      "\n",
      "Epoch [32]\t Average training loss 0.0228\t Average training accuracy 0.9905\n",
      "Epoch [32]\t Average validation loss 0.0316\t Average validation accuracy 0.9869\n",
      "\n",
      "Epoch [33][100]\t Batch [500][500]\t Training Loss 0.0369\t Accuracy 0.9800\n",
      "\n",
      "Epoch [33]\t Average training loss 0.0233\t Average training accuracy 0.9899\n",
      "Epoch [33]\t Average validation loss 0.0326\t Average validation accuracy 0.9854\n",
      "\n",
      "Epoch [34][100]\t Batch [500][500]\t Training Loss 0.0200\t Accuracy 0.9900\n",
      "\n",
      "Epoch [34]\t Average training loss 0.0235\t Average training accuracy 0.9901\n",
      "Epoch [34]\t Average validation loss 0.0335\t Average validation accuracy 0.9868\n",
      "\n",
      "Epoch [35][100]\t Batch [500][500]\t Training Loss 0.0340\t Accuracy 0.9800\n",
      "\n",
      "Epoch [35]\t Average training loss 0.0227\t Average training accuracy 0.9906\n",
      "Epoch [35]\t Average validation loss 0.0318\t Average validation accuracy 0.9871\n",
      "\n",
      "Epoch [36][100]\t Batch [500][500]\t Training Loss 0.0183\t Accuracy 0.9900\n",
      "\n",
      "Epoch [36]\t Average training loss 0.0218\t Average training accuracy 0.9908\n",
      "Epoch [36]\t Average validation loss 0.0315\t Average validation accuracy 0.9849\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37][100]\t Batch [500][500]\t Training Loss 0.0283\t Accuracy 0.9900\n",
      "\n",
      "Epoch [37]\t Average training loss 0.0238\t Average training accuracy 0.9899\n",
      "Epoch [37]\t Average validation loss 0.0344\t Average validation accuracy 0.9832\n",
      "\n",
      "Epoch [38][100]\t Batch [500][500]\t Training Loss 0.0152\t Accuracy 1.0000\n",
      "\n",
      "Epoch [38]\t Average training loss 0.0227\t Average training accuracy 0.9909\n",
      "Epoch [38]\t Average validation loss 0.0389\t Average validation accuracy 0.9826\n",
      "\n",
      "Epoch [39][100]\t Batch [500][500]\t Training Loss 0.0196\t Accuracy 0.9800\n",
      "\n",
      "Epoch [39]\t Average training loss 0.0224\t Average training accuracy 0.9910\n",
      "Epoch [39]\t Average validation loss 0.0320\t Average validation accuracy 0.9858\n",
      "\n",
      "Epoch [40][100]\t Batch [500][500]\t Training Loss 0.0185\t Accuracy 0.9900\n",
      "\n",
      "Epoch [40]\t Average training loss 0.0227\t Average training accuracy 0.9907\n",
      "Epoch [40]\t Average validation loss 0.0316\t Average validation accuracy 0.9878\n",
      "\n",
      "Epoch [41][100]\t Batch [500][500]\t Training Loss 0.0397\t Accuracy 0.9800\n",
      "\n",
      "Epoch [41]\t Average training loss 0.0235\t Average training accuracy 0.9899\n",
      "Epoch [41]\t Average validation loss 0.0312\t Average validation accuracy 0.9871\n",
      "\n",
      "Epoch [42][100]\t Batch [500][500]\t Training Loss 0.0161\t Accuracy 1.0000\n",
      "\n",
      "Epoch [42]\t Average training loss 0.0223\t Average training accuracy 0.9904\n",
      "Epoch [42]\t Average validation loss 0.0328\t Average validation accuracy 0.9855\n",
      "\n",
      "Epoch [43][100]\t Batch [500][500]\t Training Loss 0.0201\t Accuracy 0.9900\n",
      "\n",
      "Epoch [43]\t Average training loss 0.0224\t Average training accuracy 0.9910\n",
      "Epoch [43]\t Average validation loss 0.0316\t Average validation accuracy 0.9872\n",
      "\n",
      "Epoch [44][100]\t Batch [500][500]\t Training Loss 0.0113\t Accuracy 1.0000\n",
      "\n",
      "Epoch [44]\t Average training loss 0.0233\t Average training accuracy 0.9899\n",
      "Epoch [44]\t Average validation loss 0.0359\t Average validation accuracy 0.9841\n",
      "\n",
      "Epoch [45][100]\t Batch [500][500]\t Training Loss 0.0244\t Accuracy 0.9800\n",
      "\n",
      "Epoch [45]\t Average training loss 0.0224\t Average training accuracy 0.9905\n",
      "Epoch [45]\t Average validation loss 0.0351\t Average validation accuracy 0.9838\n",
      "\n",
      "Epoch [46][100]\t Batch [500][500]\t Training Loss 0.0116\t Accuracy 1.0000\n",
      "\n",
      "Epoch [46]\t Average training loss 0.0228\t Average training accuracy 0.9902\n",
      "Epoch [46]\t Average validation loss 0.0374\t Average validation accuracy 0.9818\n",
      "\n",
      "Epoch [47][100]\t Batch [500][500]\t Training Loss 0.0138\t Accuracy 0.9900\n",
      "\n",
      "Epoch [47]\t Average training loss 0.0223\t Average training accuracy 0.9906\n",
      "Epoch [47]\t Average validation loss 0.0333\t Average validation accuracy 0.9844\n",
      "\n",
      "Epoch [48][100]\t Batch [500][500]\t Training Loss 0.0199\t Accuracy 0.9900\n",
      "\n",
      "Epoch [48]\t Average training loss 0.0228\t Average training accuracy 0.9906\n",
      "Epoch [48]\t Average validation loss 0.0330\t Average validation accuracy 0.9862\n",
      "\n",
      "Epoch [49][100]\t Batch [500][500]\t Training Loss 0.0279\t Accuracy 0.9800\n",
      "\n",
      "Epoch [49]\t Average training loss 0.0221\t Average training accuracy 0.9911\n",
      "Epoch [49]\t Average validation loss 0.0325\t Average validation accuracy 0.9866\n",
      "\n",
      "Epoch [50][100]\t Batch [500][500]\t Training Loss 0.0141\t Accuracy 1.0000\n",
      "\n",
      "Epoch [50]\t Average training loss 0.0225\t Average training accuracy 0.9910\n",
      "Epoch [50]\t Average validation loss 0.0354\t Average validation accuracy 0.9844\n",
      "\n",
      "Epoch [51][100]\t Batch [500][500]\t Training Loss 0.0382\t Accuracy 0.9800\n",
      "\n",
      "Epoch [51]\t Average training loss 0.0232\t Average training accuracy 0.9902\n",
      "Epoch [51]\t Average validation loss 0.0333\t Average validation accuracy 0.9848\n",
      "\n",
      "Epoch [52][100]\t Batch [500][500]\t Training Loss 0.0280\t Accuracy 0.9900\n",
      "\n",
      "Epoch [52]\t Average training loss 0.0222\t Average training accuracy 0.9909\n",
      "Epoch [52]\t Average validation loss 0.0338\t Average validation accuracy 0.9852\n",
      "\n",
      "Epoch [53][100]\t Batch [500][500]\t Training Loss 0.0492\t Accuracy 0.9700\n",
      "\n",
      "Epoch [53]\t Average training loss 0.0225\t Average training accuracy 0.9910\n",
      "Epoch [53]\t Average validation loss 0.0317\t Average validation accuracy 0.9867\n",
      "\n",
      "Epoch [54][100]\t Batch [500][500]\t Training Loss 0.0247\t Accuracy 0.9800\n",
      "\n",
      "Epoch [54]\t Average training loss 0.0225\t Average training accuracy 0.9911\n",
      "Epoch [54]\t Average validation loss 0.0330\t Average validation accuracy 0.9868\n",
      "\n",
      "Epoch [55][100]\t Batch [500][500]\t Training Loss 0.0417\t Accuracy 0.9800\n",
      "\n",
      "Epoch [55]\t Average training loss 0.0222\t Average training accuracy 0.9913\n",
      "Epoch [55]\t Average validation loss 0.0358\t Average validation accuracy 0.9821\n",
      "\n",
      "Epoch [56][100]\t Batch [500][500]\t Training Loss 0.0335\t Accuracy 0.9800\n",
      "\n",
      "Epoch [56]\t Average training loss 0.0224\t Average training accuracy 0.9905\n",
      "Epoch [56]\t Average validation loss 0.0356\t Average validation accuracy 0.9833\n",
      "\n",
      "Epoch [57][100]\t Batch [500][500]\t Training Loss 0.0306\t Accuracy 0.9900\n",
      "\n",
      "Epoch [57]\t Average training loss 0.0224\t Average training accuracy 0.9906\n",
      "Epoch [57]\t Average validation loss 0.0315\t Average validation accuracy 0.9849\n",
      "\n",
      "Epoch [58][100]\t Batch [500][500]\t Training Loss 0.0481\t Accuracy 0.9700\n",
      "\n",
      "Epoch [58]\t Average training loss 0.0225\t Average training accuracy 0.9906\n",
      "Epoch [58]\t Average validation loss 0.0390\t Average validation accuracy 0.9831\n",
      "\n",
      "Epoch [59][100]\t Batch [500][500]\t Training Loss 0.0358\t Accuracy 0.9800\n",
      "\n",
      "Epoch [59]\t Average training loss 0.0219\t Average training accuracy 0.9905\n",
      "Epoch [59]\t Average validation loss 0.0368\t Average validation accuracy 0.9831\n",
      "\n",
      "Epoch [60][100]\t Batch [500][500]\t Training Loss 0.0095\t Accuracy 1.0000\n",
      "\n",
      "Epoch [60]\t Average training loss 0.0229\t Average training accuracy 0.9910\n",
      "Epoch [60]\t Average validation loss 0.0318\t Average validation accuracy 0.9843\n",
      "\n",
      "Epoch [61][100]\t Batch [500][500]\t Training Loss 0.0581\t Accuracy 0.9900\n",
      "\n",
      "Epoch [61]\t Average training loss 0.0222\t Average training accuracy 0.9911\n",
      "Epoch [61]\t Average validation loss 0.0436\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [62][100]\t Batch [500][500]\t Training Loss 0.0175\t Accuracy 1.0000\n",
      "\n",
      "Epoch [62]\t Average training loss 0.0218\t Average training accuracy 0.9908\n",
      "Epoch [62]\t Average validation loss 0.0341\t Average validation accuracy 0.9843\n",
      "\n",
      "Epoch [63][100]\t Batch [500][500]\t Training Loss 0.0130\t Accuracy 1.0000\n",
      "\n",
      "Epoch [63]\t Average training loss 0.0228\t Average training accuracy 0.9903\n",
      "Epoch [63]\t Average validation loss 0.0368\t Average validation accuracy 0.9827\n",
      "\n",
      "Epoch [64][100]\t Batch [500][500]\t Training Loss 0.0239\t Accuracy 0.9800\n",
      "\n",
      "Epoch [64]\t Average training loss 0.0220\t Average training accuracy 0.9911\n",
      "Epoch [64]\t Average validation loss 0.0352\t Average validation accuracy 0.9834\n",
      "\n",
      "Epoch [65][100]\t Batch [500][500]\t Training Loss 0.0162\t Accuracy 1.0000\n",
      "\n",
      "Epoch [65]\t Average training loss 0.0218\t Average training accuracy 0.9911\n",
      "Epoch [65]\t Average validation loss 0.0371\t Average validation accuracy 0.9864\n",
      "\n",
      "Epoch [66][100]\t Batch [500][500]\t Training Loss 0.0237\t Accuracy 0.9900\n",
      "\n",
      "Epoch [66]\t Average training loss 0.0225\t Average training accuracy 0.9900\n",
      "Epoch [66]\t Average validation loss 0.0321\t Average validation accuracy 0.9863\n",
      "\n",
      "Epoch [67][100]\t Batch [500][500]\t Training Loss 0.0328\t Accuracy 0.9800\n",
      "\n",
      "Epoch [67]\t Average training loss 0.0217\t Average training accuracy 0.9909\n",
      "Epoch [67]\t Average validation loss 0.0335\t Average validation accuracy 0.9837\n",
      "\n",
      "Epoch [68][100]\t Batch [500][500]\t Training Loss 0.0153\t Accuracy 1.0000\n",
      "\n",
      "Epoch [68]\t Average training loss 0.0224\t Average training accuracy 0.9908\n",
      "Epoch [68]\t Average validation loss 0.0331\t Average validation accuracy 0.9843\n",
      "\n",
      "Epoch [69][100]\t Batch [500][500]\t Training Loss 0.0183\t Accuracy 0.9900\n",
      "\n",
      "Epoch [69]\t Average training loss 0.0224\t Average training accuracy 0.9903\n",
      "Epoch [69]\t Average validation loss 0.0302\t Average validation accuracy 0.9877\n",
      "\n",
      "Epoch [70][100]\t Batch [500][500]\t Training Loss 0.0286\t Accuracy 0.9900\n",
      "\n",
      "Epoch [70]\t Average training loss 0.0224\t Average training accuracy 0.9910\n",
      "Epoch [70]\t Average validation loss 0.0326\t Average validation accuracy 0.9855\n",
      "\n",
      "Epoch [71][100]\t Batch [500][500]\t Training Loss 0.0158\t Accuracy 1.0000\n",
      "\n",
      "Epoch [71]\t Average training loss 0.0217\t Average training accuracy 0.9916\n",
      "Epoch [71]\t Average validation loss 0.0328\t Average validation accuracy 0.9845\n",
      "\n",
      "Epoch [72][100]\t Batch [500][500]\t Training Loss 0.0096\t Accuracy 1.0000\n",
      "\n",
      "Epoch [72]\t Average training loss 0.0225\t Average training accuracy 0.9910\n",
      "Epoch [72]\t Average validation loss 0.0341\t Average validation accuracy 0.9844\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73][100]\t Batch [500][500]\t Training Loss 0.0238\t Accuracy 0.9900\n",
      "\n",
      "Epoch [73]\t Average training loss 0.0223\t Average training accuracy 0.9908\n",
      "Epoch [73]\t Average validation loss 0.0343\t Average validation accuracy 0.9861\n",
      "\n",
      "Epoch [74][100]\t Batch [500][500]\t Training Loss 0.0299\t Accuracy 0.9800\n",
      "\n",
      "Epoch [74]\t Average training loss 0.0224\t Average training accuracy 0.9913\n",
      "Epoch [74]\t Average validation loss 0.0350\t Average validation accuracy 0.9851\n",
      "\n",
      "Epoch [75][100]\t Batch [500][500]\t Training Loss 0.0110\t Accuracy 1.0000\n",
      "\n",
      "Epoch [75]\t Average training loss 0.0219\t Average training accuracy 0.9905\n",
      "Epoch [75]\t Average validation loss 0.0358\t Average validation accuracy 0.9823\n",
      "\n",
      "Epoch [76][100]\t Batch [500][500]\t Training Loss 0.0200\t Accuracy 0.9900\n",
      "\n",
      "Epoch [76]\t Average training loss 0.0229\t Average training accuracy 0.9907\n",
      "Epoch [76]\t Average validation loss 0.0342\t Average validation accuracy 0.9865\n",
      "\n",
      "Epoch [77][100]\t Batch [500][500]\t Training Loss 0.0120\t Accuracy 1.0000\n",
      "\n",
      "Epoch [77]\t Average training loss 0.0217\t Average training accuracy 0.9913\n",
      "Epoch [77]\t Average validation loss 0.0350\t Average validation accuracy 0.9849\n",
      "\n",
      "Epoch [78][100]\t Batch [500][500]\t Training Loss 0.0142\t Accuracy 1.0000\n",
      "\n",
      "Epoch [78]\t Average training loss 0.0214\t Average training accuracy 0.9913\n",
      "Epoch [78]\t Average validation loss 0.0312\t Average validation accuracy 0.9867\n",
      "\n",
      "Epoch [79][100]\t Batch [500][500]\t Training Loss 0.0216\t Accuracy 0.9900\n",
      "\n",
      "Epoch [79]\t Average training loss 0.0229\t Average training accuracy 0.9904\n",
      "Epoch [79]\t Average validation loss 0.0364\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [80][100]\t Batch [500][500]\t Training Loss 0.0293\t Accuracy 0.9700\n",
      "\n",
      "Epoch [80]\t Average training loss 0.0217\t Average training accuracy 0.9907\n",
      "Epoch [80]\t Average validation loss 0.0350\t Average validation accuracy 0.9840\n",
      "\n",
      "\n",
      "Epoch [81]\t Average training loss 0.0216\t Average training accuracy 0.9914\n",
      "Epoch [81]\t Average validation loss 0.0341\t Average validation accuracy 0.9855\n",
      "\n",
      "Epoch [82][100]\t Batch [500][500]\t Training Loss 0.0288\t Accuracy 0.9800\n",
      "\n",
      "Epoch [82]\t Average training loss 0.0225\t Average training accuracy 0.9907\n",
      "Epoch [82]\t Average validation loss 0.0351\t Average validation accuracy 0.9826\n",
      "\n",
      "Epoch [83][100]\t Batch [500][500]\t Training Loss 0.0259\t Accuracy 0.9900\n",
      "\n",
      "Epoch [83]\t Average training loss 0.0229\t Average training accuracy 0.9908\n",
      "Epoch [83]\t Average validation loss 0.0334\t Average validation accuracy 0.9857\n",
      "\n",
      "Epoch [84][100]\t Batch [500][500]\t Training Loss 0.0180\t Accuracy 0.9900\n",
      "\n",
      "Epoch [84]\t Average training loss 0.0212\t Average training accuracy 0.9921\n",
      "Epoch [84]\t Average validation loss 0.0333\t Average validation accuracy 0.9878\n",
      "\n",
      "Epoch [85][100]\t Batch [500][500]\t Training Loss 0.0126\t Accuracy 1.0000\n",
      "\n",
      "Epoch [85]\t Average training loss 0.0223\t Average training accuracy 0.9906\n",
      "Epoch [85]\t Average validation loss 0.0317\t Average validation accuracy 0.9868\n",
      "\n",
      "Epoch [86][100]\t Batch [500][500]\t Training Loss 0.0340\t Accuracy 0.9700\n",
      "\n",
      "Epoch [86]\t Average training loss 0.0227\t Average training accuracy 0.9909\n",
      "Epoch [86]\t Average validation loss 0.0441\t Average validation accuracy 0.9749\n",
      "\n",
      "Epoch [87][100]\t Batch [500][500]\t Training Loss 0.0254\t Accuracy 0.9900\n",
      "\n",
      "Epoch [87]\t Average training loss 0.0216\t Average training accuracy 0.9915\n",
      "Epoch [87]\t Average validation loss 0.0345\t Average validation accuracy 0.9858\n",
      "\n",
      "Epoch [88][100]\t Batch [500][500]\t Training Loss 0.0475\t Accuracy 0.9700\n",
      "\n",
      "Epoch [88]\t Average training loss 0.0216\t Average training accuracy 0.9916\n",
      "Epoch [88]\t Average validation loss 0.0351\t Average validation accuracy 0.9853\n",
      "\n",
      "Epoch [89][100]\t Batch [500][500]\t Training Loss 0.0176\t Accuracy 0.9900\n",
      "\n",
      "Epoch [89]\t Average training loss 0.0219\t Average training accuracy 0.9908\n",
      "Epoch [89]\t Average validation loss 0.0331\t Average validation accuracy 0.9859\n",
      "\n",
      "Epoch [90][100]\t Batch [500][500]\t Training Loss 0.0382\t Accuracy 0.9800\n",
      "\n",
      "Epoch [90]\t Average training loss 0.0217\t Average training accuracy 0.9910\n",
      "Epoch [90]\t Average validation loss 0.0424\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [91][100]\t Batch [500][500]\t Training Loss 0.0181\t Accuracy 0.9900\n",
      "\n",
      "Epoch [91]\t Average training loss 0.0218\t Average training accuracy 0.9908\n",
      "Epoch [91]\t Average validation loss 0.0338\t Average validation accuracy 0.9853\n",
      "\n",
      "Epoch [92][100]\t Batch [500][500]\t Training Loss 0.0350\t Accuracy 0.9800\n",
      "\n",
      "Epoch [92]\t Average training loss 0.0225\t Average training accuracy 0.9909\n",
      "Epoch [92]\t Average validation loss 0.0355\t Average validation accuracy 0.9844\n",
      "\n",
      "Epoch [93][100]\t Batch [500][500]\t Training Loss 0.0241\t Accuracy 0.9900\n",
      "\n",
      "Epoch [93]\t Average training loss 0.0218\t Average training accuracy 0.9914\n",
      "Epoch [93]\t Average validation loss 0.0355\t Average validation accuracy 0.9842\n",
      "\n",
      "Epoch [94][100]\t Batch [500][500]\t Training Loss 0.0291\t Accuracy 0.9900\n",
      "\n",
      "Epoch [94]\t Average training loss 0.0214\t Average training accuracy 0.9913\n",
      "Epoch [94]\t Average validation loss 0.0319\t Average validation accuracy 0.9881\n",
      "\n",
      "Epoch [95][100]\t Batch [500][500]\t Training Loss 0.0180\t Accuracy 0.9900\n",
      "\n",
      "Epoch [95]\t Average training loss 0.0218\t Average training accuracy 0.9911\n",
      "Epoch [95]\t Average validation loss 0.0333\t Average validation accuracy 0.9861\n",
      "\n",
      "Epoch [96][100]\t Batch [500][500]\t Training Loss 0.0090\t Accuracy 1.0000\n",
      "\n",
      "Epoch [96]\t Average training loss 0.0225\t Average training accuracy 0.9910\n",
      "Epoch [96]\t Average validation loss 0.0329\t Average validation accuracy 0.9860\n",
      "\n",
      "Epoch [97][100]\t Batch [500][500]\t Training Loss 0.0350\t Accuracy 0.9900\n",
      "\n",
      "Epoch [97]\t Average training loss 0.0211\t Average training accuracy 0.9916\n",
      "Epoch [97]\t Average validation loss 0.0426\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [98][100]\t Batch [500][500]\t Training Loss 0.0195\t Accuracy 0.9900\n",
      "\n",
      "Epoch [98]\t Average training loss 0.0228\t Average training accuracy 0.9899\n",
      "Epoch [98]\t Average validation loss 0.0333\t Average validation accuracy 0.9857\n",
      "\n",
      "Epoch [99][100]\t Batch [500][500]\t Training Loss 0.0142\t Accuracy 1.0000\n",
      "\n",
      "Epoch [99]\t Average training loss 0.0211\t Average training accuracy 0.9912\n",
      "Epoch [99]\t Average validation loss 0.0339\t Average validation accuracy 0.9851\n",
      "\n",
      "Epoch [100][100]\t Batch [500][500]\t Training Loss 0.0281\t Accuracy 0.9800\n",
      "\n",
      "Epoch [100]\t Average training loss 0.0217\t Average training accuracy 0.9918\n",
      "Epoch [100]\t Average validation loss 0.0342\t Average validation accuracy 0.9825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args.disp_freq = 500\n",
    "model, train_loss, train_acc = solver.train_val(model, optimizer, criterion, train_loader, val_loader,\n",
    "                                                    args.max_epoch, args.disp_freq, args.cuda, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 0.9852.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_acc = solver.test_model(model, criterion, test_loader, args.cuda)\n",
    "solver.plot_loss_and_acc({'CDTree': [train_loss, train_acc]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
